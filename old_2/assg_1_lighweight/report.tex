\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[kazakh,english]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{margin=2.5cm}

\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{green!60!black},
    breaklines=true,
    frame=single
}

\title{\textbf{Терең оқыту - 1-тапсырма}\\
\large Жеңілдетілген нұсқа есебі}
\author{Deep Learning курсы}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Кіріспе}

Бұл есеп терең оқыту негіздерін қамтиды: MLP, оптимизация және CNN. Жеңілдетілген нұсқа - тез орындалу үшін эпохалар саны азайтылған.

\subsection{Эпохалар}
\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Есеп} & \textbf{Түпнұсқа} & \textbf{Жеңіл} \\
\midrule
MLP оқыту & 20 & 3 \\
XOR оқыту & 1000 & 200 \\
Оптимизаторлар & 20 & 3 \\
Градиент талдау & 10 & 2 \\
CNN оқыту & 10 & 2 \\
Transfer Learning & 10 & 2 \\
\bottomrule
\end{tabular}
\caption{Эпохалар саны}
\end{table}

\section{1-Есеп: MLP жүйке желілері}

\subsection{A бөлімі: MLP құру}

Көп қабатты перцептрон (MLP) - толық байланысқан жүйке желісі.

\textbf{Архитектура:}
\begin{itemize}
    \item Кіріс: 784 (28x28 пиксел)
    \item Жасырын қабат 1: 128 нейрон
    \item Жасырын қабат 2: 64 нейрон
    \item Шығыс: 10 класс (0-9 сандар)
\end{itemize}

\begin{lstlisting}
class MLP(nn.Module):
    def __init__(self, layer_sizes, activation='relu'):
        # layer_sizes = [784, 128, 64, 10]
        layers = []
        for i in range(len(layer_sizes) - 1):
            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))
            if i < len(layer_sizes) - 2:
                layers.append(activation)
        self.network = nn.Sequential(*layers)
\end{lstlisting}

\subsection{B бөлімі: Активация функциялары}

Үш активация функциясын салыстырдық:

\begin{itemize}
    \item \textbf{ReLU}: $f(x) = \max(0, x)$ - ең жылдам, градиент мәселесі жоқ
    \item \textbf{Sigmoid}: $f(x) = \frac{1}{1+e^{-x}}$ - градиент жоғалу мәселесі
    \item \textbf{Tanh}: $f(x) = \tanh(x)$ - sigmoid-тан жақсы, бірақ ReLU-дан баяу
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{results/part1b_loss_comparison.png}
\caption{Активация функцияларын салыстыру}
\end{figure}

\subsection{C бөлімі: XOR есебі}

XOR - сызықты емес есеп, бір қабатты перцептрон шеше алмайды.

\textbf{Шешім:} Жасырын қабат қосу (2 $\rightarrow$ 4 $\rightarrow$ 1).

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{results/part1c_xor_boundary.png}
\caption{XOR шешім шекарасы}
\end{figure}

\section{2-Есеп: Оптимизация}

\subsection{A бөлімі: Оптимизаторлар}

Төрт оптимизаторды салыстырдық:

\begin{enumerate}
    \item \textbf{SGD}: Стохастикалық градиент түсу
    \item \textbf{SGD+Momentum}: Импульспен SGD - жылдамырақ конвергенция
    \item \textbf{RMSprop}: Адаптивті оқыту жылдамдығы
    \item \textbf{Adam}: Momentum + RMSprop біріктіру
\end{enumerate}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{results/part2a_optimizer_comparison.png}
\caption{Оптимизаторларды салыстыру}
\end{figure}

\subsection{B бөлімі: Градиент мәселесі}

Терең желілерде градиенттер жоғалуы мүмкін (vanishing gradients).

\textbf{Тәжірибе:} 6 қабатты желі, Sigmoid vs ReLU.

\textbf{Нәтиже:} Sigmoid-те бірінші қабаттардың градиенттері өте кіші. ReLU-де бұл мәселе жоқ.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{results/part2b_gradient_analysis.png}
\caption{Градиент талдау}
\end{figure}

\subsection{C бөлімі: Регуляризация}

Артық оқытуды (overfitting) болдырмау әдістері:

\begin{itemize}
    \item \textbf{L2 регуляризация}: Салмақтарға айыппұл $\lambda \sum w^2$
    \item \textbf{Dropout}: Нейрондарды кездейсоқ өшіру (p=0.5)
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{results/part2c_regularization.png}
\caption{Регуляризация салыстыру}
\end{figure}

\section{3-Есеп: CNN}

\subsection{A бөлімі: Негізгі CNN}

Конволюциялық желі - суреттерге арналған архитектура.

\textbf{Архитектура:}
\begin{lstlisting}
Conv2d(3, 32, 3x3) -> ReLU -> MaxPool(2x2)
Conv2d(32, 64, 3x3) -> ReLU -> MaxPool(2x2)
Linear(4096, 128) -> ReLU
Linear(128, 10)
\end{lstlisting}

\textbf{CIFAR-10 деректері:} 10 класс (ұшақ, авто, құс, т.б.), 32x32 RGB суреттер.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{results/part3a_training_curves.png}
\caption{CNN оқыту қисықтары}
\end{figure}

\subsection{B бөлімі: Архитектура эксперименттері}

Төрт архитектураны салыстырдық:

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Модель} & \textbf{Параметр} & \textbf{Ерекшелігі} \\
\midrule
V1 (Кіші) & ~33K & Аз фильтрлер \\
V2 (Терең) & ~600K & 3 conv қабат \\
V3 (AvgPool) & ~550K & Average pooling \\
Негізгі & ~550K & Max pooling \\
\bottomrule
\end{tabular}
\caption{CNN архитектуралары}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{results/part3b_architecture_comparison.png}
\caption{Архитектура салыстыру}
\end{figure}

\subsection{C бөлімі: Визуализация}

\textbf{Фильтрлер:} Бірінші conv қабаты түстер мен жиектерді анықтайды.

\textbf{Активациялар:} Әр фильтр суреттің әр түрлі бөлігіне реакция береді.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{results/part3c_filters.png}
\caption{Conv1 фильтрлері}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{results/part3c_activations.png}
\caption{Активация карталары}
\end{figure}

\section{Бонус: Transfer Learning}

\subsection{Идея}

ImageNet-те оқытылған ResNet18 моделін CIFAR-10-ге бейімдеу.

\textbf{Әдіс:}
\begin{enumerate}
    \item ResNet18 салмақтарын жүктеу (ImageNet)
    \item Барлық қабаттарды тоңдыру
    \item Соңғы FC қабатты ауыстыру (1000 $\rightarrow$ 10)
    \item Тек соңғы қабатты оқыту
\end{enumerate}

\begin{lstlisting}
model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)
for param in model.parameters():
    param.requires_grad = False  # Тондыру
model.fc = nn.Linear(512, 10)    # Жана кабат
\end{lstlisting}

\subsection{Нәтижелер}

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Модель} & \textbf{Параметр} & \textbf{Дәлдік} \\
\midrule
Негізгі CNN & 550K & ~55\% \\
Transfer Learning & 5K (оқытылатын) & ~80\% \\
\bottomrule
\end{tabular}
\caption{Transfer Learning салыстыру}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{results/bonus_transfer_learning.png}
\caption{Transfer Learning нәтижелері}
\end{figure}

\section{Қорытынды}

\begin{enumerate}
    \item \textbf{MLP}: Қарапайым есептер үшін жеткілікті, ReLU ең жақсы активация
    \item \textbf{Оптимизация}: Adam - ең жылдам және тұрақты оптимизатор
    \item \textbf{CNN}: Суреттер үшін MLP-ден әлдеқайда жақсы
    \item \textbf{Transfer Learning}: Аз деректермен жоғары нәтиже
\end{enumerate}

\end{document}
