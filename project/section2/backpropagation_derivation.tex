\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{margin=1in}

\title{Backpropagation Mathematical Derivation}
\author{Deep Learning Project}
\date{}

\begin{document}

\maketitle

\section{Overview}
This document provides a complete mathematical derivation of the backpropagation algorithm for a 2-layer neural network.

\section{Network Architecture}

Consider a 2-layer neural network:
\begin{itemize}
    \item \textbf{Input layer}: $x \in \mathbb{R}^{d}$ (input vector of dimension $d$)
    \item \textbf{Hidden layer}: $h \in \mathbb{R}^{m}$ (hidden layer with $m$ neurons)
    \item \textbf{Output layer}: $\hat{y} \in \mathbb{R}^{k}$ (output vector of dimension $k$)
\end{itemize}

\subsection{Forward Pass}

\textbf{Layer 1 (Input to Hidden):}
\begin{align}
z_1 &= W_1 x + b_1 \\
h &= \sigma(z_1)
\end{align}

where:
\begin{itemize}
    \item $W_1 \in \mathbb{R}^{m \times d}$ is the weight matrix
    \item $b_1 \in \mathbb{R}^{m}$ is the bias vector
    \item $\sigma$ is the activation function (e.g., ReLU, sigmoid, tanh)
\end{itemize}

\textbf{Layer 2 (Hidden to Output):}
\begin{align}
z_2 &= W_2 h + b_2 \\
\hat{y} &= \text{softmax}(z_2)
\end{align}

where:
\begin{itemize}
    \item $W_2 \in \mathbb{R}^{k \times m}$ is the weight matrix
    \item $b_2 \in \mathbb{R}^{k}$ is the bias vector
    \item $\text{softmax}$ is the softmax activation function
\end{itemize}

\subsection{Loss Function}

For multi-class classification, we use cross-entropy loss:
\begin{equation}
L = -\sum_{i=1}^{k} y_i \log(\hat{y}_i)
\end{equation}

where $y$ is the one-hot encoded true label.

\section{Backpropagation Derivation}

\subsection{Step 1: Gradient w.r.t. Output Layer}

We start by computing the gradient of the loss with respect to the output logits $z_2$:

\begin{equation}
\frac{\partial L}{\partial z_2} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z_2}
\end{equation}

\textbf{Gradient of loss w.r.t. softmax output:}
\begin{equation}
\frac{\partial L}{\partial \hat{y}_i} = -\frac{y_i}{\hat{y}_i}
\end{equation}

\textbf{Gradient of softmax w.r.t. logits:}
\begin{equation}
\frac{\partial \hat{y}_i}{\partial z_{2,j}} = \begin{cases}
\hat{y}_i (1 - \hat{y}_i) & \text{if } i = j \\
-\hat{y}_i \hat{y}_j & \text{if } i \neq j
\end{cases}
\end{equation}

Combining these:
\begin{align}
\frac{\partial L}{\partial z_{2,i}} &= \sum_{j=1}^{k} \frac{\partial L}{\partial \hat{y}_j} \cdot \frac{\partial \hat{y}_j}{\partial z_{2,i}} \\
&= -\sum_{j=1}^{k} \frac{y_j}{\hat{y}_j} \cdot \frac{\partial \hat{y}_j}{\partial z_{2,i}} \\
&= -\frac{y_i}{\hat{y}_i} \cdot \hat{y}_i (1 - \hat{y}_i) + \sum_{j \neq i} \frac{y_j}{\hat{y}_j} \cdot \hat{y}_i \hat{y}_j \\
&= -y_i (1 - \hat{y}_i) + \sum_{j \neq i} y_j \hat{y}_i \\
&= -y_i + y_i \hat{y}_i + \sum_{j \neq i} y_j \hat{y}_i \\
&= -y_i + \hat{y}_i \sum_{j=1}^{k} y_j
\end{align}

Since $\sum_{j=1}^{k} y_j = 1$ (one-hot encoding):
\begin{equation}
\frac{\partial L}{\partial z_2} = \hat{y} - y
\end{equation}

\subsection{Step 2: Gradient w.r.t. $W_2$ and $b_2$}

\textbf{Gradient w.r.t. $W_2$:}
\begin{equation}
\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial z_2}{\partial W_2}
\end{equation}

Since $z_2 = W_2 h + b_2$:
\begin{equation}
\frac{\partial z_2}{\partial W_2} = h^T
\end{equation}

Therefore:
\begin{equation}
\frac{\partial L}{\partial W_2} = (\hat{y} - y) h^T
\end{equation}

\textbf{Gradient w.r.t. $b_2$:}
\begin{equation}
\frac{\partial L}{\partial b_2} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial z_2}{\partial b_2} = \hat{y} - y
\end{equation}

\subsection{Step 3: Gradient w.r.t. Hidden Layer}

\textbf{Gradient w.r.t. $h$:}
\begin{equation}
\frac{\partial L}{\partial h} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial z_2}{\partial h}
\end{equation}

Since $z_2 = W_2 h + b_2$:
\begin{equation}
\frac{\partial z_2}{\partial h} = W_2^T
\end{equation}

Therefore:
\begin{equation}
\frac{\partial L}{\partial h} = W_2^T (\hat{y} - y)
\end{equation}

\subsection{Step 4: Gradient w.r.t. $z_1$}

\textbf{Gradient w.r.t. $z_1$:}
\begin{equation}
\frac{\partial L}{\partial z_1} = \frac{\partial L}{\partial h} \cdot \frac{\partial h}{\partial z_1}
\end{equation}

Since $h = \sigma(z_1)$:
\begin{equation}
\frac{\partial h}{\partial z_1} = \sigma'(z_1)
\end{equation}

where $\sigma'$ is the derivative of the activation function.

Therefore:
\begin{equation}
\frac{\partial L}{\partial z_1} = W_2^T (\hat{y} - y) \odot \sigma'(z_1)
\end{equation}

where $\odot$ denotes element-wise multiplication.

\subsection{Step 5: Gradient w.r.t. $W_1$ and $b_1$}

\textbf{Gradient w.r.t. $W_1$:}
\begin{equation}
\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial z_1} \cdot \frac{\partial z_1}{\partial W_1}
\end{equation}

Since $z_1 = W_1 x + b_1$:
\begin{equation}
\frac{\partial z_1}{\partial W_1} = x^T
\end{equation}

Therefore:
\begin{equation}
\frac{\partial L}{\partial W_1} = [W_2^T (\hat{y} - y) \odot \sigma'(z_1)] x^T
\end{equation}

\textbf{Gradient w.r.t. $b_1$:}
\begin{equation}
\frac{\partial L}{\partial b_1} = \frac{\partial L}{\partial z_1} = W_2^T (\hat{y} - y) \odot \sigma'(z_1)
\end{equation}

\section{Summary of Gradients}

For a 2-layer network with cross-entropy loss and softmax output:

\begin{enumerate}
    \item \textbf{Output layer gradients:}
    \begin{itemize}
        \item $\frac{\partial L}{\partial z_2} = \hat{y} - y$
        \item $\frac{\partial L}{\partial W_2} = (\hat{y} - y) h^T$
        \item $\frac{\partial L}{\partial b_2} = \hat{y} - y$
    \end{itemize}
    
    \item \textbf{Hidden layer gradients:}
    \begin{itemize}
        \item $\frac{\partial L}{\partial h} = W_2^T (\hat{y} - y)$
        \item $\frac{\partial L}{\partial z_1} = W_2^T (\hat{y} - y) \odot \sigma'(z_1)$
        \item $\frac{\partial L}{\partial W_1} = [W_2^T (\hat{y} - y) \odot \sigma'(z_1)] x^T$
        \item $\frac{\partial L}{\partial b_1} = W_2^T (\hat{y} - y) \odot \sigma'(z_1)$
    \end{itemize}
\end{enumerate}

\section{Activation Function Derivatives}

\textbf{ReLU:}
\begin{align}
\sigma(x) &= \max(0, x) \\
\sigma'(x) &= \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}
\end{align}

\textbf{Sigmoid:}
\begin{align}
\sigma(x) &= \frac{1}{1 + e^{-x}} \\
\sigma'(x) &= \sigma(x)(1 - \sigma(x))
\end{align}

\textbf{Tanh:}
\begin{align}
\sigma(x) &= \tanh(x) \\
\sigma'(x) &= 1 - \tanh^2(x) = 1 - \sigma^2(x)
\end{align}

\section{Implementation Notes}

\begin{enumerate}
    \item The gradients are computed in reverse order (output to input)
    \item The chain rule is applied at each step
    \item Element-wise operations (like $\odot$) are crucial for correct computation
    \item For mini-batch training, gradients are averaged over the batch
\end{enumerate}

\section{References}

\begin{itemize}
    \item Rumelhart, D. E., Hinton, G. E., \& Williams, R. J. (1986). Learning representations by back-propagating errors. \textit{Nature}, 323(6088), 533-536.
    \item Goodfellow, I., Bengio, Y., \& Courville, A. (2016). \textit{Deep Learning}. MIT Press.
\end{itemize}

\end{document}
